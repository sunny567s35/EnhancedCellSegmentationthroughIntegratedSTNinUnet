{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7348419,"sourceType":"datasetVersion","datasetId":4267108},{"sourceId":7728235,"sourceType":"datasetVersion","datasetId":4515430},{"sourceId":7743448,"sourceType":"datasetVersion","datasetId":4526253},{"sourceId":7743451,"sourceType":"datasetVersion","datasetId":4526256}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nDeep Learning has been very successful in solving many complex problems in the real world. However, solving most of these challenges requires data. Additionally, having data is not just enough. The data has to be cleansed, annotated, and organized. Moreover, data in many domains is not accessible due to privacy constraints (Especially in the medical domain). Hence it becomes pertinent to create models that can learn from a limited amount of data.\n\n<br>\n\n## Problem Statement:\nIn this task, you are given a small dataset of the microscopic view of the cells.\n\n*Your goal is to build a model that accurately predicts the cell regions as shown in the **Label Image**.*\n\n#### **Input Image**\n\n<a href=\"https://imgur.com/aLDNHwu\"><img src=\"https://i.imgur.com/aLDNHwu.png\" title=\"source: imgur.com\" /></a>\n\n#### **Label Image**\n\n<a href=\"https://imgur.com/s1mIkFE\"><img src=\"https://i.imgur.com/s1mIkFE.png\" title=\"source: imgur.com\" /></a>\n\n\n## Dataset:\n\nThe dataset contains 30 training images along with the labels.\n\n[**Dataset Link**](https://drive.google.com/drive/folders/1678Tggykj46SpJZS9mKMKHw7YFmiGMc8?usp=sharing)\n\n\n## Evaluation\n\nShare the submission Jupyter/Colab notebook with the relevant explanation. code and models. We will evaluate your model on our **test dataset**.\n\n<br><br>\n****Note that this is an interview assignment & is not involved in the development of any software or solution.**","metadata":{"id":"CjwC8T6KuCCt"}},{"cell_type":"markdown","source":"### Instructions\n\n1. Make a copy of this notebook to start editing & add your solution.\n2. A dataset folder has been shared with you to train and test.\n2. You can use any framework to develop the solution (Pytorch, Keras, Tensorflow, Theano, Caffe etc.).\n3. This assignment is a great medium to get to know you better. Please feel free to connect, interact & develop the solution. I would be more than willing to help you out in any issues or problems that you face while solving the challenge. You can connect with me at suraj.donthi@elementals.ai.\n4. The goal of this task is to understand how you approach solving a problem. The more you connect while developing the solution the better I will be able to understand you.\n5. Submission Files:\n    - Colab Notebook Link with Solutions Approach and Code Solution.\n    - Trained Model Link.\n    - Any other necessary files.\n5. **Submission deadline: Within 5 days of recieving the assignment, no later than 12 AM IST on the due date. The exact due date shall be mentioned in the email or the portal where you recieve the assignment.** (You can share the Colab Notebook to the gmail address surajdonthi.th@gmail.com)","metadata":{"id":"Qc6vK_ej5338"}},{"cell_type":"markdown","source":"## Explain the technique you will use to solve the problem in detail.\n\n- Include any model architectures, equations, diagrams etc. that is required to explain how you are going to solve the problem.","metadata":{"id":"gNmRTPRZ181E"}},{"cell_type":"markdown","source":"# New Section","metadata":{"id":"qPnOr-Nm5Wlv"}},{"cell_type":"markdown","source":"\n# Cell Segmentation using U-Net: Solution Explanation\n\nIn this assignment, the task is to build a model that accurately predicts cell regions in microscopic images. We are given a small dataset of microscopic cell images along with their corresponding labeled masks. The goal is to develop a deep learning model that can segment the cells in the images.\n\nHere's a detailed explanation of the approach taken to solve this problem:\n\n## Model Selection: U-Net Architecture\nBefore jumping into the solution as a good practice ,I've referred few research papers (A deep learning-based algorithm for 2-D cell segmentation in microscopy images,U-Net: Convolutional Networks for Biomedical Image Segmentation) to get a good understaing of the starting point for this solution.\n\n Once the first step is cleared I've found that U-net which is a variant of FCNN which is best suitable for these kind of tasks ,keeping in mind the limited dataset provided.\nThe U-Net architecture is a popular choice for image segmentation tasks due to its ability to capture both high-level and low-level features. It consists of an encoder-decoder structure with skip connections, making it effective for precise pixel-wise segmentation. Two variations of the U-Net architecture are provided in the code: `UNet()` and `UNet2()`. Both architectures follow the same basic U-Net structure but differ in the number of filters used in each layer. The `UNet2()` architecture uses larger filter sizes, which can capture more complex features.So it is retained in the final notebook.\n\nHere's an explanation of the architecture:\n\n1. **Input Layer**:\n   - The model takes grayscale images as input, and the input shape is `(image_size, image_size, 1)`, where `image_size` is the size of input image (512 x 512).\n\n2. **Encoder (Downsampling Path)**:\n   - The encoder consists of a series of down blocks, where each down block performs the following operations:\n     - Convolutional Layer: Applies a 2D convolution operation with a specified number of filters and a ReLU activation function. This layer is responsible for capturing features in the input image.\n     - Convolutional Layer (Again): Another convolution operation is applied to further enhance feature extraction.\n     - Max Pooling: Downsamples the feature maps using max-pooling with a pool size of (2, 2). This operation reduces the spatial dimensions of the feature maps.\n     - Dropout: Applies dropout regularization to the pooled feature maps to prevent overfitting.\n\n   - The number of filters in the convolutional layers increases as we go deeper into the encoder (from `f[0]` to `f[3]` in the architecture).\n\n3. **Bottleneck**:\n   - After the encoder, there is a bottleneck layer. This layer typically has the highest number of filters (`f[4]`) and is intended to capture the most abstract features in the image.\n\n4. **Decoder (Upsampling Path)**:\n   - The decoder consists of a series of up blocks, where each up block performs the following operations:\n     - UpSampling: Increases the spatial dimensions of the feature maps by a factor of 2 using up-sampling.\n     - Concatenation: Concatenates the up-sampled feature maps with the corresponding feature maps from the encoder's down block (skip connections).\n     - Convolutional Layer: Applies a 2D convolution operation to the concatenated feature maps with a ReLU activation.\n     - Convolutional Layer (Again): Another convolution operation is applied for further feature refinement.\n     - Dropout: Applies dropout regularization.\n\n   - The number of filters in the convolutional layers decreases as we go deeper into the decoder (from `f[3]` to `f[0]` in the architecture).\n\n5. **Output Layer**:\n   - The final output layer consists of a 2D convolutional layer with a single filter and a sigmoid activation function. This layer produces the predicted binary masks, where each pixel value represents the probability that the corresponding pixel in the input image belongs to the object of interest (e.g., a cell nucleus).\n\nIn summary, this U-Net architecture combines both the encoder and decoder paths, with skip connections between corresponding encoder and decoder blocks. This allows the model to capture and retain fine details while simultaneously learning high-level features, making it well-suited for image segmentation tasks like cell segmentation. The model outputs binary masks that can be used to identify and segment objects of interest in input images.\n\nhere is the table showing the changes in the image dimensions after passing through each layer .\n\n| Layer        | Input Size (H x W) | Output Size (H x W) |\n|----------------|-----------------------------|----------------------|\n| Input          | 512 x 512               | 512 x 512         |\n| Down Block 1   | 512 x 512           | 256 x 256    |\n| Down Block 2   | 256 x 256           | 128 x 128    |\n| Down Block 3   | 128 x 128           | 64 x 64         |\n| Down Block 4   | 64 x 64             | 32 x 32           |\n| Bottleneck     | 32 x 32            \t | 32 x 32          |\n| Up Block 1     | 32 x 32            \t | 64 x 64          |\n| Up Block 2     | 64 x 64             \t| 128 x 128       |\n| Up Block 3     | 128 x 128           | 256 x 256        |\n| Up Block 4     | 256 x 256           | 512 x 512        |\n| Output (Sigmoid)| 512 x 512           | 512 x 512 |\n\n## Data Preprocessing:\n\n### Data Augmentation:\n\nData augmentation techniques such as rotation, scaling, and flipping have been applied to artificially increase the size of the training dataset and improve model generalization. However, in this implementation, data augmentation techniques are not explicitly shown,I've performed it in another notebook which I'll attach with the link for the notebook. This is done to ease the process of training and tuning of the model.\n\n### Data Loading:\n\nA custom data generator class (`DataGen`) is implemented to load and preprocess the dataset. This class inherits from `keras.utils.Sequence` and provides batch-wise data loading. The following steps are performed during data loading:\n\n- Load both the input image and its corresponding labeled mask.\n- Resize both the image and mask to a specified image size (512x512 in this case).\n- Apply histogram equalization to enhance image contrast.\n- Normalize the pixel values of both the image and mask to the range [0, 1].\n\n## Model Training:\n\n### Loss Function and Metrics:\n\nThe model is trained using the binary cross-entropy loss function, which is commonly used for binary image segmentation tasks. The optimizer chosen for training is Adam. Additionally, accuracy is used as a metric to monitor the model's performance during training.\n\n### Training Data Split:\n\nThe dataset is split into a training set and a validation set. In this implementation, 60 images are used for training, and 10 images are reserved for validation. You can adjust these numbers as needed.\n\n### Training Loop:\n\nThe model is intially set to be trained for 150 epochs with \"early stopping\" enabled for a patience value of \"10\" . For each epoch, the training data is divided into batches, and the model's weights are updated using backpropagation. The validation data are used to monitor the model's performance and prevent overfitting.\n\n## Model Evaluation:\n\n### Evaluation Metrics:\n\nTo evaluate the model's performance, two evaluation metrics are used:\n\n1. Intersection over Union (IoU): IoU measures the overlap between the predicted mask and the ground truth mask. It is a commonly used metric for image segmentation tasks and provides a measure of segmentation accuracy.\n\n2. Warping Error: Warping error quantifies the absolute pixel-wise difference between the predicted and ground truth masks. It is used to assess how well the predicted mask aligns with the ground truth mask.\n\n### Visualization:\n\nFor each validation sample, the following steps are performed:\n\n- Load the ground truth mask.\n- Get the predicted mask from the model.\n- Performed Morphological Closing\n- Binarized the predicted mask using a threshold (0.6 ).\n- Calculate both IoU and warping error.\n- Visualize the ground truth mask and the predicted mask side by side for visual inspection.\n\n## Model Save:\n\nAfter training, the model's weights are saved to a file named \"UNetW_assessment.h5\" for future use.\n\n## Conclusion:\n\nThe U-Net architecture is a powerful choice for image segmentation tasks like cell segmentation usually . This implementation provides a structured approach to loading, preprocessing, training, and evaluating the model. The combination of binary cross-entropy loss, Adam optimizer, and evaluation metrics like IoU and warping error helps in assessing and improving the model's performance.\n\n\n\n\n\n\n================== `Your answer here. (Double click to edit)` ==================","metadata":{"id":"ieBjQssnPC0h"}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Input, Concatenate,Dropout,Flatten ,Reshape\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import MeanIoU\nfrom tensorflow.keras.layers import Lambda\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Input, Concatenate, Dropout, Flatten, Dense\nfrom tensorflow.keras.models import Model\n\nfrom tensorflow.keras.layers import Lambda\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\nfrom tensorflow.keras.metrics import MeanIoU","metadata":{"id":"No5qVlLqiyiv","execution":{"iopub.status.busy":"2024-03-02T12:19:26.383870Z","iopub.execute_input":"2024-03-02T12:19:26.384330Z","iopub.status.idle":"2024-03-02T12:19:42.111092Z","shell.execute_reply.started":"2024-03-02T12:19:26.384296Z","shell.execute_reply":"2024-03-02T12:19:42.110045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Definining dataset path and other parameters/kaggle/input/ds-cell/dataset1980\ndataset_path = '/kaggle/input/ds-cell/dataset1980'\nimage_size = 512\nbatch_size = 16\nepochs = 150\n","metadata":{"id":"ws_WIeTQB0MV","execution":{"iopub.status.busy":"2024-02-29T14:52:48.555646Z","iopub.execute_input":"2024-02-29T14:52:48.556312Z","iopub.status.idle":"2024-02-29T14:52:48.561048Z","shell.execute_reply.started":"2024-02-29T14:52:48.556277Z","shell.execute_reply":"2024-02-29T14:52:48.559910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Model arch-10\n# Defining the U-Net model architecture\ndef down_block(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1,dropout_rate=0.2):\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    p = MaxPooling2D((2, 2), (2, 2))(c)\n    p = Dropout(dropout_rate)(p)\n    return c, p\n\ndef up_block(x, skip, filters, kernel_size=(3, 3), padding=\"same\", strides=1,dropout_rate=0.2):\n    us = UpSampling2D((2, 2))(x)\n    concat = Concatenate()([us, skip])\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    c = Dropout(dropout_rate)(c)\n    return c\n\ndef bottleneck(x, filters, kernel_size=(3, 3), padding=\"same\", strides=1,dropout_rate=0.2):\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n    c = Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(c)\n    c = Dropout(dropout_rate)(c)\n    return c\n","metadata":{"id":"Hq0LLb8GB1yL","execution":{"iopub.status.busy":"2024-02-29T14:52:51.633356Z","iopub.execute_input":"2024-02-29T14:52:51.634091Z","iopub.status.idle":"2024-02-29T14:52:51.643669Z","shell.execute_reply.started":"2024-02-29T14:52:51.634058Z","shell.execute_reply":"2024-02-29T14:52:51.642772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# STN Function\ndef spatial_transformer_network(input_layer):\n    loc_net = Conv2D(8, (3, 3), activation='relu')(input_layer)\n    loc_net = MaxPooling2D(pool_size=(2, 2))(loc_net)\n    loc_net = Conv2D(10, (3, 3), activation='relu')(loc_net)\n    loc_net = MaxPooling2D(pool_size=(2, 2))(loc_net)\n\n    loc_net = Flatten()(loc_net)\n    loc_net = Dense(50, activation='relu')(loc_net)\n    loc_net = Dense(8, weights=[np.zeros((50, 8)), np.zeros(8)])(loc_net)  # Change 6 to 8\n\n    loc_net = Reshape((2, 4))(loc_net)  # Change 3 to 4\n\n    # Flatten the loc_net tensor to make it rank 1\n    loc_net_flat = Flatten()(loc_net)\n\n    x = Lambda(lambda args: tfa.image.transform(args[0], args[1]))([input_layer, loc_net_flat])\n\n    return x\n\n# Dfining the U-Net model architecture with STN\ndef UNet2_with_STN():\n    f = [64, 128, 256, 512, 1024]\n    inputs = Input((image_size, image_size, 1))\n\n    # Apply STN to the input\n    stn_output = spatial_transformer_network(inputs)\n\n    # Concatenate STN output with the original input\n    inputs_transformed = Concatenate()([inputs, stn_output])\n\n    p0 = inputs_transformed\n    c1, p1 = down_block(p0, f[0])\n    c2, p2 = down_block(p1, f[1])\n    c3, p3 = down_block(p2, f[2])\n    c4, p4 = down_block(p3, f[3])\n\n    bn = bottleneck(p4, f[4])\n\n    u1 = up_block(bn, c4, f[3])\n    u2 = up_block(u1, c3, f[2])\n    u3 = up_block(u2, c2, f[1])\n    u4 = up_block(u3, c1, f[0])\n\n    outputs = Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u4)\n    model = Model(inputs, outputs)\n    return model\n","metadata":{"id":"OU2992guTesG","execution":{"iopub.status.busy":"2024-02-29T14:52:52.927262Z","iopub.execute_input":"2024-02-29T14:52:52.927629Z","iopub.status.idle":"2024-02-29T14:52:52.943427Z","shell.execute_reply.started":"2024-02-29T14:52:52.927600Z","shell.execute_reply":"2024-02-29T14:52:52.942348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass DataGen(keras.utils.Sequence):\n    def __init__(self, ids, path, batch_size=16, image_size=512):\n        self.ids = ids\n        self.path = path\n        self.batch_size = batch_size\n        self.image_size = image_size\n        self.on_epoch_end()\n\n    def __load__(self, id_name):\n        ## Path\n        image_path = os.path.join(self.path, 'trainvolume/', id_name)\n        mask_name = f'train-labels_{id_name.split(\"_\")[1]}'\n        mask_path = os.path.join(self.path, 'trainlabels/', mask_name)\n\n        # Reading Image\n        image = cv2.imread(image_path, 0)\n        image = cv2.resize(image, (self.image_size, self.image_size))\n\n#         Loading the label image as grayscale\n        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n        mask = cv2.resize(mask, (self.image_size, self.image_size))\n        mask = np.expand_dims(mask, axis=-1)\n            # Histogram equalization\n        # image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        image = cv2.equalizeHist(image)\n        ## Normalizing\n        image = image / 255.0\n        mask = mask / 255.0\n\n        return image, mask\n\n    def __getitem__(self, index):\n        if (index + 1) * self.batch_size > len(self.ids):\n            self.batch_size = len(self.ids) - index * self.batch_size\n\n        files_batch = self.ids[index * self.batch_size: (index + 1) * self.batch_size]\n\n        image = []\n        mask = []\n\n        for id_name in files_batch:\n            _img, _mask = self.__load__(id_name)\n            image.append(_img)\n            mask.append(_mask)\n\n        image = np.array(image)\n        mask = np.array(mask)\n\n        return image, mask\n\n    def on_epoch_end(self):\n        pass\n\n    def __len__(self):\n        return int(np.ceil(len(self.ids) / float(self.batch_size)))\n","metadata":{"id":"Z1ZsxLuVB2GF","execution":{"iopub.status.busy":"2024-02-29T14:52:56.647323Z","iopub.execute_input":"2024-02-29T14:52:56.647694Z","iopub.status.idle":"2024-02-29T14:52:56.659842Z","shell.execute_reply.started":"2024-02-29T14:52:56.647664Z","shell.execute_reply":"2024-02-29T14:52:56.658830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\n## Training Ids\ntrain_ids = [f'train-volume_{str(i)}.tif' for i in range(1,1981)]\n## Validation Data Size\nnp.random.shuffle(train_ids)\n\nval_data_size = 576\nvalid_ids = train_ids[:val_data_size]\ntrain_ids = train_ids[val_data_size:]\nnp.random.shuffle(valid_ids)\nnp.random.shuffle(train_ids)\n\ngen = DataGen(train_ids, dataset_path, batch_size=batch_size, image_size=image_size)\nvalid_gen = DataGen(valid_ids, dataset_path, batch_size=batch_size, image_size=image_size)\n# Define and compile the U-Net model\nstrategy = tf.distribute.MirroredStrategy()\nwith strategy.scope():\n\n    model = UNet2_with_STN()\n    \n    mean_iou = MeanIoU(num_classes=2) \n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\",mean_iou])\n    \n# model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\ntrain_steps = len(train_ids) // batch_size\nvalid_steps = len(valid_ids) // batch_size\n\nearly_stopping = EarlyStopping(monitor='val_loss',  # Monitor validation loss\n                               patience=10,         # Number of epochs with no improvement to wait before stopping\n                               restore_best_weights=True)  # Restore model weights to the best observed during training\n\nmodel.fit(gen, validation_data=valid_gen, steps_per_epoch=train_steps,\n          validation_steps=valid_steps, epochs=epochs, callbacks=[early_stopping])\n# Saving the trained model weights\nmodel.save_weights(\"UNet+stn_model_finalcall.h5\")","metadata":{"id":"fsdPBJlSjvQT","outputId":"edc7f8ad-f900-4ff9-c9a2-6ff7c3b3388d","execution":{"iopub.status.busy":"2024-02-29T15:27:36.332217Z","iopub.execute_input":"2024-02-29T15:27:36.333063Z","iopub.status.idle":"2024-02-29T18:04:50.658849Z","shell.execute_reply.started":"2024-02-29T15:27:36.333028Z","shell.execute_reply":"2024-02-29T18:04:50.657983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef intersection_over_union(y_true, y_pred):\n    # Resize predicted mask to match the shape of true mask\n    y_pred_resized = cv2.resize(y_pred, (y_true.shape[1], y_true.shape[0]))\n    \n    # Convert masks to binary images\n    y_true_binary = (y_true > 0).astype(np.uint8)\n    y_pred_binary = (y_pred_resized > 0).astype(np.uint8)\n\n    # Perform element-wise multiplication\n    intersection = np.sum(y_true_binary * y_pred_binary)\n    union = np.sum(y_true_binary) + np.sum(y_pred_binary) - intersection\n    iou = intersection / (union + 1e-8)  # Adding a small epsilon to avoid division by zero\n\n    return iou\n","metadata":{"id":"FUQoK7wHN2lk","execution":{"iopub.status.busy":"2024-02-29T18:49:35.182799Z","iopub.execute_input":"2024-02-29T18:49:35.183225Z","iopub.status.idle":"2024-02-29T18:49:35.190090Z","shell.execute_reply.started":"2024-02-29T18:49:35.183192Z","shell.execute_reply":"2024-02-29T18:49:35.189165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef calculate_warping_error(true_image, warped_image):\n    # Ensure both images have the same shape\n    if true_image.shape != warped_image.shape:\n        return 0\n    if len(true_image.shape) == 3:\n        true_image = cv2.cvtColor(true_image, cv2.COLOR_BGR2GRAY)\n    if len(warped_image.shape) == 3:\n        warped_image = cv2.cvtColor(warped_image, cv2.COLOR_BGR2GRAY)\n\n    diff = np.abs(true_image.astype(np.float32) - warped_image.astype(np.float32))\n\n    warping_error = np.mean(diff)\n    \n    return warping_error\n\n","metadata":{"id":"NOOzA0tUPdwb","execution":{"iopub.status.busy":"2024-02-29T18:53:46.853154Z","iopub.execute_input":"2024-02-29T18:53:46.853548Z","iopub.status.idle":"2024-02-29T18:53:46.861521Z","shell.execute_reply.started":"2024-02-29T18:53:46.853509Z","shell.execute_reply":"2024-02-29T18:53:46.860470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef plot_masks(true_mask, pred_mask):\n\n    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n\n    # Plot the ground truth mask on the first subplot\n    axes[0].imshow(true_mask, cmap='gray')\n    axes[0].set_title('Ground Truth Mask')\n    axes[0].axis('off')\n\n    # Plot the predicted mask on the second subplot\n    axes[1].imshow(pred_mask, cmap='gray')\n    axes[1].set_title('Predicted Mask')\n    axes[1].axis('off')\n\n    plt.show()","metadata":{"id":"1HCvSTI9Q8vF","execution":{"iopub.status.busy":"2024-02-29T18:53:47.344825Z","iopub.execute_input":"2024-02-29T18:53:47.345160Z","iopub.status.idle":"2024-02-29T18:53:47.351513Z","shell.execute_reply.started":"2024-02-29T18:53:47.345133Z","shell.execute_reply":"2024-02-29T18:53:47.350548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredicted_masks = model.predict(valid_gen)\n\nkernel = np.ones((1, 1), np.uint8)\n\nprocessed_masks = []\n\nfor i in range(len(predicted_masks)):\n    pred_mask = predicted_masks[i].squeeze()\n\n    processed_mask = cv2.morphologyEx(pred_mask, cv2.MORPH_CLOSE, kernel)\n\n    processed_masks.append(processed_mask)\n\nprocessed_masks = np.array(processed_masks)\n\niou_scores = []\n\nfor i in range(len(valid_ids)):\n    # Loading the ground truth mask\n    mask_name = f'train-labels_{valid_ids[i].split(\"_\")[1]}'\n    mask_path = os.path.join(dataset_path, 'trainlabels/', mask_name)\n    true_mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n    true_mask = (true_mask > 128).astype(np.uint8)\n\n    pred_mask = processed_masks[i]\n    pred_mask = (pred_mask > 0.5).astype(np.uint8)  # Binarizing the predicted mask\n\n    iou = intersection_over_union(true_mask, pred_mask)\n    \n    iou_scores.append(iou)\n    if i %64 ==0 :\n        plot_masks(true_mask,pred_mask )\n        print(\"IoU for one sample:\", iou)\n        print(\"Warping Error:\", warping_error)\n    warping_error = calculate_warping_error(true_mask, pred_mask)\n\n# Calculating the mean IoU across all validation samples\nmean_iou = np.mean(iou_scores)\nprint(f\"Mean IoU: {mean_iou}\")\n","metadata":{"id":"aL57L0B1xSaz","execution":{"iopub.status.busy":"2024-02-29T18:53:48.342822Z","iopub.execute_input":"2024-02-29T18:53:48.343161Z","iopub.status.idle":"2024-02-29T18:54:27.597151Z","shell.execute_reply.started":"2024-02-29T18:53:48.343133Z","shell.execute_reply":"2024-02-29T18:54:27.596086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# serializing model to JSON\nmodel_json = model.to_json()\nwith open(\"model_final.json\", \"w\") as json_file:\n    json_file.write(model_json)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-29T18:36:27.279989Z","iopub.execute_input":"2024-02-29T18:36:27.280372Z","iopub.status.idle":"2024-02-29T18:36:27.307288Z","shell.execute_reply.started":"2024-02-29T18:36:27.280347Z","shell.execute_reply":"2024-02-29T18:36:27.306490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mean IoU: 0.9651099867947772 - keranlsize = (1,1) - threshold value = 0.6\n# Mean IOu : 0.97500000000000 - Kernalsize = (1,1) - threshold value = 0.5\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}